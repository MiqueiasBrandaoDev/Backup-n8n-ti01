{"updatedAt":"2025-05-09T14:08:10.998Z","createdAt":"2025-05-08T13:18:51.720Z","id":"Zo1TkS8fXHqMFixN","name":"RAG SEMANTICO","active":false,"isArchived":false,"nodes":[{"parameters":{"jsCode":"// ‚úÖ Paso 1: Obtener el texto desde la variable de entrada JSON.\n// Si no existe, usamos un string vac√≠o para evitar errores.\nconst text = $json.text || '';\n\n// ‚úÖ Paso 2: Limpiar el texto para facilitar el procesamiento.\n// - normalize(\"NFD\"): separa letras y tildes (ej. \"√°\" -> \"a\" + tilde).\n// - replace: elimina saltos de l√≠nea (\\n), retornos (\\r), tabulaciones (\\t) y espacios duplicados.\nconst cleanText = text.normalize(\"NFD\")\n  .replace(/[\\n\\r\\t]+/g, ' ')\n  .replace(/\\s{2,}/g, ' ');\n\n// ‚úÖ Paso 3: Dividir el texto en frases.\n// Usamos una expresi√≥n regular que detecta final de frase (., ?, ! seguidos de espacio).\nconst sentences = cleanText.split(/(?<=[.?!])\\s+/);\n\n// ‚úÖ Paso 4: Preparar variables\nconst chunks = [];         // Aqu√≠ guardaremos los fragmentos resultantes\nlet currentChunk = [];     // Acumula frases hasta llegar al l√≠mite\nlet charCursor = 0;        // Nos ayuda a encontrar la posici√≥n exacta del fragmento en el texto original\n\n// ‚úÖ Paso 5: Recorrer todas las frases una a una\nfor (let i = 0; i < sentences.length; i++) {\n  const sentence = sentences[i];\n  currentChunk.push(sentence); // A√±adir frase al bloque actual\n\n  // Si el bloque tiene al menos 3 frases y supera los 500 caracteres, lo cortamos aqu√≠\n  if (currentChunk.length >= 3 || currentChunk.join(\" \").length > 500) {\n    const chunkText = currentChunk.join(\" \");           // Convertimos el bloque en texto completo\n    const searchText = chunkText.slice(0, 30);          // Cogemos los primeros 30 caracteres para buscarlo en el texto original\n    const matchPos = cleanText.indexOf(searchText, charCursor); // Buscamos el fragmento en el texto original desde la √∫ltima posici√≥n\n\n    const startIndex = matchPos > -1 ? matchPos : null; // Si lo encontramos, guardamos la posici√≥n\n\n    // Guardamos el bloque en la lista de chunks\n    chunks.push({\n      chunk: chunkText,\n      startIndex: startIndex,\n    });\n\n    // Avanzamos el cursor si encontramos la posici√≥n\n    if (matchPos > -1) charCursor = matchPos + chunkText.length;\n\n    // Reseteamos el bloque actual para seguir con el siguiente\n    currentChunk = [];\n  }\n}\n\n// ‚úÖ Paso 6: Guardar el √∫ltimo fragmento que haya quedado pendiente\nif (currentChunk.length) {\n  const chunkText = currentChunk.join(\" \");\n  const searchText = chunkText.slice(0, 30);\n  const matchPos = cleanText.indexOf(searchText, charCursor);\n  const startIndex = matchPos > -1 ? matchPos : null;\n\n  chunks.push({\n    chunk: chunkText,\n    startIndex: startIndex,\n  });\n}\n\n// ‚úÖ Paso 7: Devolver los resultados en el formato que espera n8n\n// Cada fragmento se envuelve dentro de un objeto { json: ... }\nreturn chunks.map(c => ({ json: c }));"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[1820,240],"id":"b7a6be08-39ab-46d9-ad36-7acd065e4731","name":"Semantic Chunker"},{"parameters":{"method":"POST","url":"=https://aasupabaseaa.vps.rubikmarketing.com.co/rest/v1/documents_RAG_Semantico","authentication":"predefinedCredentialType","nodeCredentialType":"supabaseApi","sendHeaders":true,"headerParameters":{"parameters":[{"name":"Content-Type","value":"application/json"},{"name":"Prefer","value":"return=representation"}]},"sendBody":true,"specifyBody":"json","jsonBody":"={\n  \"content\": \"{{ $('Code2').item.json.chunk.replaceAll('\\\"','') }}\",\n  \"metadata\": {\n    \"file_name\": \"{{ $('Google Drive1').item.json.message.content.title.replaceAll('\\\"','') }}\",\n    \"title\": \"{{ $('Code2').item.json.originalInput.message.content.title }}\",\n    \"section\": \"{{ $('Code2').item.json.originalInput.message.content.section }}\",\n    \"subsection\": \"{{ $('Code2').item.json.originalInput.message.content.subsection }}\",\n    \"pages\": \"{{ $('Extract from File1').item.json.numpages }}\",\n    \"id_file\": \"{{ $('Google Drive').item.json.id }}\"\n  },\n  \"embedding\": {{ JSON.stringify($json.data[0].embedding) }}\n}","options":{}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[3160,320],"id":"501143a8-7eb7-4b02-b7d8-6fa077ff1962","name":"HTTP Request2"},{"parameters":{"options":{}},"type":"n8n-nodes-base.splitInBatches","typeVersion":3,"position":[2040,240],"id":"d882489e-67c4-45bc-854f-519016416dc3","name":"Loop Over Items"},{"parameters":{"operation":"download","fileId":{"__rl":true,"value":"={{ $json.id }}","mode":"id"},"options":{}},"type":"n8n-nodes-base.googleDrive","typeVersion":3,"position":[680,240],"id":"a83d2cb3-9ea7-454c-9977-f11052c31901","name":"Google Drive","credentials":{"googleDriveOAuth2Api":{"id":"UMUzXd6peuAPf6IA","name":"Google Drive account"}}},{"parameters":{"operation":"pdf","options":{}},"type":"n8n-nodes-base.extractFromFile","typeVersion":1,"position":[940,240],"id":"c4174a50-d311-468c-b977-e98a92dde30a","name":"Extract from File1"},{"parameters":{"modelId":{"__rl":true,"value":"gpt-4.1","mode":"list","cachedResultName":"GPT-4.1"},"messages":{"values":[{"content":"=Analiza este documento y devu√©lveme un resumen jer√°rquico en JSON de su estructura. Para cada bloque, identifica:\n\n- `title`: t√≠tulo general del documento\n- `section`: secci√≥n principal\n- `subsection`: subt√≠tulo o parte espec√≠fica\n- `start`: √≠ndice del texto donde inicia esa secci√≥n\n- `end`: √≠ndice del texto donde termina\n\n_ Siempre debes arrancar por el primer caracter\nTexto completo:\n{{ $('Extract from File1').item.json.text }}\n"}]},"jsonOutput":true,"options":{}},"type":"@n8n/n8n-nodes-langchain.openAi","typeVersion":1.8,"position":[1120,240],"id":"79af4683-6941-482f-ac49-612b119a5436","name":"OpenAI"},{"parameters":{"modelId":{"__rl":true,"value":"gpt-4.1","mode":"list","cachedResultName":"GPT-4.1"},"messages":{"values":[{"content":"=Tienes la estructura de una base de conocimiento que contiene:\n\n- Un √∫nico t√≠tulo general.\n- Varias secciones y subsecciones.\n- Cada secci√≥n y subsecci√≥n tiene un rango de posiciones (`start`, `end`) que indican el lugar del texto completo donde aparecen.\n\nA continuaci√≥n, se te proporciona un fragmento de texto (`chunk`) con su √≠ndice de inicio (`startIndex`) respecto al texto original.\n\nTu tarea es: analizar el contenido del `chunk` y su posici√≥n (`startIndex`) y devolver exactamente a cu√°l secci√≥n y subsecci√≥n le corresponde, seg√∫n la estructura.\n\n‚úÖ Devuelve solo un JSON en este formato:\n\n{\n  \"title\": \"T√≠tulo exacto\",\n  \"section\": \"Nombre exacto de la secci√≥n\",\n  \"subsection\": \"Nombre exacto de la subsecci√≥n o null si no aplica\"\n}\n\n‚ùó Usa **los nombres exactos** que aparecen en la estructura. No inventes nuevos.\n\n---\n\nüìÑ Estructura (JSON):\n\n{{ $('OpenAI').item.json.message.content.toJsonString() }}\n\n---\n\nüß© Chunk:\n\n{{ $('Semantic Chunker').item.json.chunk }}\n\nüß≠ startIndex del chunk: {{ $('Semantic Chunker').item.json.startIndex }}\n"}]},"jsonOutput":true,"options":{}},"type":"@n8n/n8n-nodes-langchain.openAi","typeVersion":1.8,"position":[2280,320],"id":"5e6f6c49-aed0-4986-9ac9-037650c444d0","name":"OpenAI1"},{"parameters":{"jsCode":"// 1. Obtener todos los items de entrada desde el nodo anterior\nconst items = $input.all();\n\n// 2. Inicializar el array de salida\nconst outputItems = [];\n\n// 3. Procesar cada item\nfor (const item of items) {\n  try {\n    const inputJson = item.json;\n\n    // Asegurarse de que el campo input existe y tiene la estructura esperada\n    if (!inputJson || !inputJson.message || typeof inputJson.message.content !== 'string') {\n      throw new Error(\"La estructura 'message.content' no se encontr√≥ o no es un string.\");\n    }\n\n    // Parsear el contenido JSON de 'message.content'\n    const parsedContent = JSON.parse(inputJson.message.content);\n\n    // Obtener el valor original del campo 'chunk' (puede venir de un nodo anterior)\n    const originalChunk = inputJson.chunk !== undefined ? inputJson.chunk : null;\n\n    // Construir el nuevo objeto JSON de salida\n    const newOutputJson = {\n      ...parsedContent,\n      chunk: originalChunk\n    };\n\n    // A√±adirlo al array de salida\n    outputItems.push({ json: newOutputJson });\n\n  } catch (error) {\n    // Capturar errores y generar una salida de error clara\n    outputItems.push({\n      json: {\n        error: \"Fallo al procesar el item\",\n        details: error.message,\n        originalInput: item.json\n      }\n    });\n  }\n}\n\n// 4. Devolver todos los objetos transformados\nreturn outputItems;"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2620,320],"id":"9b17190b-8206-4b9a-a2de-62a3ca81d65f","name":"Code1"},{"parameters":{"resource":"fileFolder","returnAll":true,"filter":{"driveId":{"mode":"list","value":"My Drive"},"folderId":{"__rl":true,"value":"1_bPUq_IIlsQYwSFUd_cZOpqSK6NDSYzp","mode":"list","cachedResultName":"RAG test","cachedResultUrl":"https://drive.google.com/drive/folders/1_bPUq_IIlsQYwSFUd_cZOpqSK6NDSYzp"}},"options":{}},"type":"n8n-nodes-base.googleDrive","typeVersion":3,"position":[420,240],"id":"17deb580-1e96-483a-9aa7-be769c43ecd6","name":"Google Drive2","credentials":{"googleDriveOAuth2Api":{"id":"UMUzXd6peuAPf6IA","name":"Google Drive account"}}},{"parameters":{"operation":"pdf","options":{}},"type":"n8n-nodes-base.extractFromFile","typeVersion":1,"position":[1640,240],"id":"bf18223f-638d-48cc-b19a-712184cabaa9","name":"Extract from File"},{"parameters":{"operation":"download","fileId":{"__rl":true,"value":"={{ $('Google Drive').item.json.id }}","mode":"id"},"options":{}},"type":"n8n-nodes-base.googleDrive","typeVersion":3,"position":[1460,240],"id":"66896d46-6a51-4567-88ec-3fef800854dd","name":"Google Drive1"},{"parameters":{"method":"POST","url":"https://api.openai.com/v1/embeddings","authentication":"predefinedCredentialType","nodeCredentialType":"openAiApi","sendHeaders":true,"headerParameters":{"parameters":[{"name":"Content-Type","value":"application/json"}]},"sendBody":true,"bodyParameters":{"parameters":[{"name":"input","value":"={{ $('Loop Over Items').item.json.chunk }}"},{"name":"model","value":"text-embedding-3-small"},{"name":"encoding_format","value":"float"},{"name":"dimensions","value":"={{1536}}"}]},"options":{}},"type":"n8n-nodes-base.httpRequest","typeVersion":4.2,"position":[2980,320],"id":"1cccaa8b-6767-4ab3-a2dc-de48e6a46733","name":"Tranformar embedding"},{"parameters":{"jsCode":"// ‚úÖ 1) Funci√≥n para limpiar texto antes de enviarlo a la API\n// Esta funci√≥n se asegura de que el chunk no tenga s√≠mbolos extra√±os, comillas problem√°ticas o caracteres invisibles.\nconst cleanText = (text) => {\n  if (typeof text !== 'string') return ''; // Si no es un string, devolvemos vac√≠o\n\n  return text\n    .replace(/[\\n\\r\\t]+/g, ' ')             // Elimina saltos de l√≠nea, retornos y tabulaciones\n    .replace(/[\"\"¬´¬ª'']/g, ' ')              // Reemplaza comillas dobles, comillas angulares y simples\n    .replace(/[‚Äì‚Äî]/g, '-')                  // Reemplaza guiones largos por guiones normales\n    .replace(/[‚Ä¢¬∑]/g, '-')                  // Reemplaza bullets por guiones\n    .replace(/\\s{2,}/g, ' ')                // Reemplaza espacios m√∫ltiples por un solo espacio\n    .replace(/\\\\(?=[^\"\\\\/bfnrtu])|\\\\$/g, '')// Elimina barras invertidas inv√°lidas en JSON\n    .trim();                                // Quita espacios al principio y al final\n};\n\n// ‚úÖ 2) Recorremos cada item del flujo (cada chunk generado anteriormente)\nreturn items.map(item => {\n  // Obtenemos el chunk crudo desde el nodo anterior llamado \"Loop Over Items\"\n  const rawChunk = $node[\"Loop Over Items\"].json.chunk;\n\n  // ‚úÖ 3) Limpiamos el chunk usando la funci√≥n anterior\n  const cleaned = cleanText(rawChunk);\n\n  // ‚úÖ 4) Escapamos el texto para que sea v√°lido en JSON:\n  // - JSON.stringify escapa todo (comillas, saltos, etc.)\n  // - Luego le quitamos las comillas exteriores que JSON.stringify a√±ade al principio y final\n  const escaped = JSON.stringify(cleaned).slice(1, -1);\n\n  // ‚úÖ 5) Guardamos el texto limpio y escapado en el campo `chunk` del item actual\n  item.json.chunk = escaped;\n\n  // Devolvemos el item actualizado\n  return item;\n});\n\n"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[2800,320],"id":"ec28197d-55e5-4b53-9857-5249cb9a9ab7","name":"Code2"},{"parameters":{"options":{}},"type":"@n8n/n8n-nodes-langchain.chatTrigger","typeVersion":1.1,"position":[180,240],"id":"745adb70-13a0-44ab-b038-bc7c6a217d52","name":"When chat message received","webhookId":"780fd289-37fa-47c0-a4cc-f90426162f85"}],"connections":{"Semantic Chunker":{"main":[[{"node":"Loop Over Items","type":"main","index":0}]]},"HTTP Request2":{"main":[[{"node":"Loop Over Items","type":"main","index":0}]]},"Loop Over Items":{"main":[[],[{"node":"OpenAI1","type":"main","index":0}]]},"Google Drive":{"main":[[{"node":"Extract from File1","type":"main","index":0}]]},"Extract from File1":{"main":[[{"node":"OpenAI","type":"main","index":0}]]},"OpenAI":{"main":[[{"node":"Google Drive1","type":"main","index":0}]]},"OpenAI1":{"main":[[{"node":"Code1","type":"main","index":0}]]},"Code1":{"main":[[{"node":"Code2","type":"main","index":0}]]},"Google Drive2":{"main":[[{"node":"Google Drive","type":"main","index":0}]]},"Extract from File":{"main":[[{"node":"Semantic Chunker","type":"main","index":0}]]},"Google Drive1":{"main":[[{"node":"Extract from File","type":"main","index":0}]]},"Tranformar embedding":{"main":[[{"node":"HTTP Request2","type":"main","index":0}]]},"Code2":{"main":[[{"node":"Tranformar embedding","type":"main","index":0}]]},"When chat message received":{"main":[[{"node":"Google Drive2","type":"main","index":0}]]}},"settings":{"executionOrder":"v1"},"staticData":null,"meta":null,"pinData":{},"versionId":"fc0d904a-a1ef-4388-bea9-a041ca1087c5","triggerCount":0,"shared":[{"updatedAt":"2025-05-08T13:18:51.720Z","createdAt":"2025-05-08T13:18:51.720Z","role":"workflow:owner","workflowId":"Zo1TkS8fXHqMFixN","projectId":"BlSED80HESP48veq"}],"tags":[{"updatedAt":"2025-05-09T14:08:08.320Z","createdAt":"2025-05-09T14:08:08.320Z","id":"saJOlgEwuuF3REbW","name":"RAG"}]}